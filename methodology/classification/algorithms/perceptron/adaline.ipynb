{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "# Description of methodology\n# Classification algorithms\n# ADAptive LInear NEuron (Adaline)\nFrom [Python Machine Learning - Second Edition By Sebastian Raschka, Vahid Mirjalili](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning-second-edition)\n\nIn this section, we will take a look at another type of single-layer neural network: **ADAptive LInear NEuron (Adaline)**. Adaline was published by Bernard Widrow and his doctoral student Tedd Hoff, only a few years after Frank Rosenblatt\u0027s perceptron algorithm, and can be considered as an improvement on the latter. (Refer to [An Adaptive \"Adaline\" Neuron Using Chemical \"Memistors\", Technical Report Number 1553-2, B. Widrow and others](http://www-isl.stanford.edu/~widrow/papers/t1960anadaptive.pdf), Stanford Electron Labs, Stanford, CA, October 1960)."
    },
    {
      "cell_type": "markdown",
      "source": "## Minimizing continuous cost functions\nThe Adaline algorithm is particularly interesting because it illustrates the key concepts of defining and minimizing continuous cost functions. This lays the groundwork for understanding more advanced machine learning algorithms for classification, such as logistic regression, support vector machines, and regression models.\n\nThe **key difference between the Adaline** rule (also known as the Widrow-Hoff rule) and Rosenblatt\u0027s perceptron is that the **weights are updated based on a linear activation function rather than a unit step function** like in the perceptron. In Adaline, this linear activation function $\\phi(z)$ is simply the identity function of the net input, so that:\n\n$ \\phi ( \\boldsymbol{w}^T \\boldsymbol{x} ) \u003d \\boldsymbol{w}^T \\boldsymbol{x} $\n\nWhile the linear activation function is used for learning the weights, we still use a threshold function to make the final prediction, which is similar to the unit step function that we have seen earlier. The main differences between the perceptron and Adaline algorithm are highlighted in the following figure:\n\n![adaline](img/adaline_dag.jpg)\n\nThe illustration shows that the Adaline algorithm compares the true class labels with the linear activation function\u0027s continuous valued output to compute the model error and update the weights. In contrast, the perceptron compares the true class labels to the predicted class labels.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Minimizing cost functions with gradient descent\nOne of the key ingredients of supervised machine learning algorithms is a defined **objective function** that is to be optimized during the learning process. This objective function is often a cost function that we want to minimize. In the case of Adaline, we can define the cost function $J$ to learn the weights as the **Sum of Squared Errors (SSE)** between the calculated outcome and the true class label:\n\n$ J ( \\boldsymbol{w} ) \u003d \\frac{1} {2} \\sum_i \\left( y^{(i)} - \\phi \\left( z^{(i)} \\right) \\right) ^2 $\n\nThe term $\\frac{1} {2}$ is just added for our convenience, which will make it easier to derive the gradient, as we will see in the following paragraphs. **The main advantage of this continuous linear activation function, in contrast to the unit step function, is that the cost function becomes differentiable.** Another nice property of this cost function is that **it is convex**; thus, we can use a simple yet powerful optimization algorithm called gradient descent to find the weights that minimize our cost function to classify the samples in the Iris dataset.\n\nAs illustrated in the following figure, we can describe the main idea behind gradient descent as climbing down a hill until a local or global cost minimum is reached. In each iteration, we take a step in the opposite direction of the gradient where the step size is determined by the value of the learning rate, as well as the slope of the gradient:\n\n![grad_desc](img/grad_desc.jpg)\n\nUsing gradient descent, we can now update the weights by taking a step in the opposite direction of the gradient $\\nabla J(w)$ of our cost function:\n\n$ \\boldsymbol{w} :\u003d \\boldsymbol{w} + \\Delta \\boldsymbol{w} $\n\nWhere the weight change $\\Delta \\boldsymbol{w}$ is defined as the negative gradient multiplied by the learning rate $\\eta$:\n\n$ \\Delta \\boldsymbol{w} \u003d - \\eta \\nabla J (\\boldsymbol{w}) $\n\nTo compute the gradient of the cost function, we need to compute the partial derivative of the cost function with respect to each weight $w_j$:\n\n$ \\frac{ \\partial{J} } { \\partial{w_j} } \u003d - \\sum \\limits_i \\left( y^{(i)} - \\phi \\left( z^{(i)} \\right) \\right) x_j^{(i)} $",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "The full derivation can be obtained using the Chain Rule of differentiation:\n\n$\\frac{ \\partial{J} } { \\partial{w_j} } \u003d\n\\frac{\\partial} {\\partial{w_j}} \\frac{1} {2} \\sum \\limits_i \\left( y^{(i)} - \\phi \\left( z^{(i)} \\right) \\right) ^2 \u003d\n\\frac{1} {2} \\frac{\\partial} {\\partial{w_j}} \\sum \\limits_i \\left( y^{(i)} - \\phi \\left( z^{(i)} \\right) \\right) ^2 \u003d\n\\frac{1} {2} \\sum \\limits_i 2 \\left( y^{(i)} - \\phi \\left( z^{(i)} \\right) \\right) \\frac{\\partial} {\\partial{w_j}} \\left( y^{(i)} - \\phi \\left( z^{(i)} \\right) \\right) \u003d\n\\sum \\limits_i \\left( y^{(i)} - \\phi \\left( z^{(i)} \\right) \\right) \\frac{\\partial} {\\partial{w_j}} \\left( y^{(i)} - \\sum \\limits_i \\left( w_j^{(i)} x_j^{(i)} \\right) \\right) \u003d\n\\sum \\limits_i \\left( y^{(i)} - \\phi \\left( z^{(i)} \\right) \\right) \\left( - x_j^{(i)} \\right) \u003d\n-\\sum \\limits_i \\left( y^{(i)} - \\phi \\left( z^{(i)} \\right) \\right) x_j^{(i)}$",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "\nSo that we can write the update of weight $w_j$ as:\n\n$ \\Delta w_j \u003d - \\eta \\frac{ \\partial{J} } { \\partial{w_j} } \u003d \\eta \\sum \\limits_i \\left( y^{(i)} - \\phi \\left( z^{(i)} \\right) \\right) x_j^{(i)}$\n\nSince we update all weights simultaneously, our Adaline learning rule becomes:\n\n$ \\boldsymbol{w} :\u003d \\boldsymbol{w} + \\Delta \\boldsymbol{w} $",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Although the Adaline learning rule looks identical to the perceptron rule, we should note that the $\\phi \\left( z^{(i)} \\right)$ with  is a real number and not an integer class label. Furthermore, the weight update is calculated based on all samples in the training set (instead of updating the weights incrementally after each sample), which is why this approach is also referred to as **batch gradient descent**.\n## Implementing Adaline in Python\nSince the perceptron rule and Adaline are very similar, we will take the perceptron implementation that we defined earlier and change the `fit` method so that the weights are updated by minimizing the cost function via gradient descent.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}